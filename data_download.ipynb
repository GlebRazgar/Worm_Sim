{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'uint64',\n",
       " 'num_channels': 1,\n",
       " 'scales': [{'chunk_sizes': [[512, 512, 16]],\n",
       "   'encoding': 'raw',\n",
       "   'key': '2.0_2.0_30.0',\n",
       "   'resolution': [2.0, 2.0, 30.0],\n",
       "   'size': [14336, 11776, 880],\n",
       "   'voxel_offset': [0, 0, 0]},\n",
       "  {'chunk_sizes': [[512, 512, 16]],\n",
       "   'encoding': 'raw',\n",
       "   'key': '4.0_4.0_30.0',\n",
       "   'resolution': [4.0, 4.0, 30.0],\n",
       "   'size': [7168, 5888, 880],\n",
       "   'voxel_offset': [0, 0, 0]},\n",
       "  {'chunk_sizes': [[512, 512, 16]],\n",
       "   'encoding': 'raw',\n",
       "   'key': '8.0_8.0_30.0',\n",
       "   'resolution': [8.0, 8.0, 30.0],\n",
       "   'size': [3584, 2944, 880],\n",
       "   'voxel_offset': [0, 0, 0]},\n",
       "  {'chunk_sizes': [[512, 512, 16]],\n",
       "   'encoding': 'raw',\n",
       "   'key': '16.0_16.0_30.0',\n",
       "   'resolution': [16.0, 16.0, 30.0],\n",
       "   'size': [1792, 1472, 880],\n",
       "   'voxel_offset': [0, 0, 0]},\n",
       "  {'chunk_sizes': [[512, 512, 16]],\n",
       "   'encoding': 'raw',\n",
       "   'key': '32.0_32.0_30.0',\n",
       "   'resolution': [32.0, 32.0, 30.0],\n",
       "   'size': [896, 736, 880],\n",
       "   'voxel_offset': [0, 0, 0]}],\n",
       " 'type': 'segmentation'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cloudvolume\n",
    "from cloudvolume import CloudVolume, Bbox\n",
    "import numpy as np\n",
    "\n",
    "sem = CloudVolume('s3://bossdb-open-data/witvliet2020/Dataset_5/em/', mip=4, secrets=None, use_https=True,fill_missing=True)\n",
    "sem_labels = CloudVolume('s3://bossdb-open-data/witvliet2020/Dataset_5/segmentation/', mip=4, secrets=None, use_https=True,fill_missing=True)\n",
    "sem.refresh_info()\n",
    "sem_labels.refresh_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "# Assert that the Zarr version is 2.*\n",
    "assert zarr.__version__.startswith('2.'), f\"Zarr version must be 2: {zarr.__version__}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- SEM (nm) ----\n",
      "Resolution at mip=0: [2.0, 2.0, 30.0]\n",
      "Resolution at mip=1: [4.0, 4.0, 30.0]\n",
      "Resolution at mip=2: [8.0, 8.0, 30.0]\n",
      "Resolution at mip=3: [16.0, 16.0, 30.0]\n",
      "Resolution at mip=4: [32.0, 32.0, 30.0]\n",
      "Resolution at mip=5: [64.0, 64.0, 30.0]\n",
      "---- SEM Labels (nm) ----\n",
      "Resolution at mip=0: [2.0, 2.0, 30.0]\n",
      "Resolution at mip=1: [4.0, 4.0, 30.0]\n",
      "Resolution at mip=2: [8.0, 8.0, 30.0]\n",
      "Resolution at mip=3: [16.0, 16.0, 30.0]\n",
      "Resolution at mip=4: [32.0, 32.0, 30.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"---- SEM (nm) ----\")\n",
    "# Print resolutions of sem at all mip levels\n",
    "for mip_level in range(len(sem.scales)):\n",
    "    resolution = sem.scales[mip_level]['resolution']\n",
    "    print(f\"Resolution at mip={mip_level}: {resolution}\")\n",
    "\n",
    "print(\"---- SEM Labels (nm) ----\")\n",
    "# Print resolutions of sem_labels at all mip levels\n",
    "for mip_level in range(len(sem_labels.scales)):\n",
    "    resolution = sem_labels.scales[mip_level]['resolution']\n",
    "    print(f\"Resolution at mip={mip_level}: {resolution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounds: Bbox([0, 0, 0],[864, 704, 864], dtype=np.int32, unit='vx')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bounds: {sem.bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zarr\n",
    "from cloudvolume import CloudVolume, Bbox\n",
    "import zarr.storage\n",
    "\n",
    "def download_and_save_as_zarr(em_volume: CloudVolume, seg_volume: CloudVolume, save_path: str, mip_level=4, squeeze_c_dim=True):\n",
    "    \"\"\"\n",
    "    Downloads two cloud volumes (EM and segmentation) at their bounding boxes,\n",
    "    finds the intersection if the bounding boxes are mismatched, and saves\n",
    "    the resulting subvolume data in a Zarr directory store (with .zarray, .zgroup, etc.).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    em_volume : CloudVolume\n",
    "        The CloudVolume instance for the EM data to download.\n",
    "    seg_volume : CloudVolume\n",
    "        The CloudVolume instance for the segmentation data to download.\n",
    "    save_path : str\n",
    "        The path to the directory where the Zarr store will be saved.\n",
    "    \"\"\"\n",
    "    # 1) Get full bounding boxes for EM and segmentation\n",
    "    em_bbox = Bbox.from_delta(em_volume.bounds.minpt, em_volume.bounds.size3())\n",
    "    seg_bbox = Bbox.from_delta(seg_volume.bounds.minpt, seg_volume.bounds.size3())\n",
    "\n",
    "    # 2) Calculate the intersection of the bounding boxes\n",
    "    intersection_bbox = em_bbox.intersection(em_bbox, seg_bbox)\n",
    "\n",
    "    # 3) Download data from the intersection region\n",
    "    em_data = em_volume.download(intersection_bbox)\n",
    "    seg_data = seg_volume.download(intersection_bbox)\n",
    "\n",
    "    # 4) Convert to numpy arrays if necessary and print dtype and check if not all zero\n",
    "    if not isinstance(em_data, np.ndarray):\n",
    "        em_data = np.array(em_data)\n",
    "    print(f\"EM data dtype: {em_data.dtype}\")\n",
    "    if np.any(em_data != 0):\n",
    "        print(\"EM data is not all zero.\")\n",
    "    \n",
    "    if not isinstance(seg_data, np.ndarray):\n",
    "        seg_data = np.array(seg_data)\n",
    "    print(f\"Segmentation data dtype: {seg_data.dtype}\")\n",
    "    if np.any(seg_data != 0):\n",
    "        print(\"Segmentation data is not all zero. ✅\")\n",
    "\n",
    "    # 5) Create a standard Zarr directory store (which includes .zarray, .zgroup, etc.)\n",
    "    store = zarr.DirectoryStore(save_path)\n",
    "    root = zarr.group(store=store, overwrite=True)\n",
    "    \n",
    "    print(\"Creating arrays\")\n",
    "    \n",
    "    # 6) Create datasets with chunked layout (adjust chunks as desired)\n",
    "    # Adjust chunk sizes to avoid exceeding buffer size limits\n",
    "\n",
    "    # Assert that the channel axis is 1 and remove it if present, only if ndim is 4 and squeeze_c is True\n",
    "    if squeeze_c_dim and em_data.ndim == 4:\n",
    "        assert em_data.shape[-1] == 1, \"EM data channel axis is not 1\"\n",
    "        em_data = em_data.squeeze(axis=-1)\n",
    "    \n",
    "    if squeeze_c_dim and seg_data.ndim == 4:\n",
    "        assert seg_data.shape[-1] == 1, \"Segmentation data channel axis is not 1\"\n",
    "        seg_data = seg_data.squeeze(axis=-1)\n",
    "\n",
    "    raw_array = root.create_dataset('raw', data=em_data, shape=em_data.shape, dtype=em_data.dtype)\n",
    "    seg_array = root.create_dataset('gt_labels', data=seg_data, shape=seg_data.shape, dtype=seg_data.dtype)\n",
    "    \n",
    "    # 7) Extract metadata for the specified mip level\n",
    "    em_info = em_volume.info['scales'][mip_level]\n",
    "    seg_info = seg_volume.info['scales'][mip_level]\n",
    "    \n",
    "    # 8) Create .zattrs for metadata using extracted values\n",
    "    raw_array.attrs.update({\n",
    "        'description': 'Raw electron microscopy data',\n",
    "        'encoding': em_info['encoding'],\n",
    "        'offset': em_info['voxel_offset'],\n",
    "        'axis_names': ['x', 'y', 'z'],\n",
    "        'units': ['nm', 'nm', 'nm'],\n",
    "        'voxel_size': em_info['resolution']\n",
    "    })\n",
    "    seg_array.attrs.update({\n",
    "        'description': 'Ground truth labels for segmentation',\n",
    "        'encoding': seg_info['encoding'],\n",
    "        'offset': seg_info['voxel_offset'],\n",
    "        'axis_names': ['x', 'y', 'z'],\n",
    "        'units': ['nm', 'nm', 'nm'],\n",
    "        'voxel_size': seg_info['resolution']\n",
    "    })\n",
    "\n",
    "    print(f\"Zarr datasets saved in '{save_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and saving to sem.zarr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 216/216 [02:25<00:00,  1.48it/s]\n",
      "Downloading: 100%|██████████| 216/216 [01:52<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM data dtype: uint8\n",
      "EM data is not all zero.\n",
      "Segmentation data dtype: uint64\n",
      "Segmentation data is not all zero. ✅\n",
      "Creating arrays\n",
      "Zarr datasets saved in 'data_neuroglancer/sem.zarr'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# The problem is that to_download contains individual volumes, not pairs\n",
    "# Let's restructure this to create proper pairs\n",
    "to_download = [(sem, sem_labels)]  # This creates a list of tuples\n",
    "zarrs = [\"sem.zarr\"]  # Adjust to match the length of to_download\n",
    "break_array = [False]  # Adjust to match the length of to_download\n",
    "base_download_path = \"data_neuroglancer\"\n",
    "\n",
    "# Now the unpacking will work correctly\n",
    "for (em, labels), target_path, toggle_break in zip(to_download, zarrs, break_array):\n",
    "    if not toggle_break:\n",
    "        print(f\"Downloading and saving to {target_path}\")\n",
    "        download_and_save_as_zarr(em, labels, save_path=os.path.join(base_download_path, target_path), squeeze_c_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import os \n",
    "def reorder_xyz_to_zyx(zarr_path):\n",
    "    print(f\"Opening Zarr store at path: {zarr_path}\")\n",
    "    store = zarr.open(zarr_path, mode='r+')\n",
    "    \n",
    "    # recursively handle groups\n",
    "    def _reorder_group(group):\n",
    "        print(f\"Processing group: {group.name}\")\n",
    "        for k, v in group.items():\n",
    "            if isinstance(v, zarr.Group):\n",
    "                print(f\"Found subgroup: {k}\")\n",
    "                _reorder_group(v)\n",
    "            elif isinstance(v, zarr.Array):\n",
    "                print(f\"Found array: {k}\")\n",
    "                axes = v.attrs.get('axis_names', None)\n",
    "                print(f\"Current axes: {axes}\")\n",
    "                if axes == ['x', 'y', 'z']:\n",
    "                    print(f\"Reordering array: {k}\")\n",
    "                    data = v[...]\n",
    "                    # reorder\n",
    "                    data = np.transpose(data, (2,1,0))\n",
    "                    # update data & axes\n",
    "                    v.resize(data.shape)\n",
    "                    v[...] = data\n",
    "                    v.attrs['axis_names'] = ['z', 'y', 'x']\n",
    "                    v.attrs['offset'] = [v.attrs['offset'][2], v.attrs['offset'][1], v.attrs['offset'][0]]\n",
    "                    v.attrs['voxel_size'] = [v.attrs['voxel_size'][2], v.attrs['voxel_size'][1], v.attrs['voxel_size'][0]]\n",
    "                    v.attrs['units'] = [v.attrs['units'][2], v.attrs['units'][1], v.attrs['units'][0]]\n",
    "                    v.attrs['axis_names'] = ['z', 'y', 'x']\n",
    "                    print(f\"Reordered axes: {v.attrs['axis_names']}\")\n",
    "    \n",
    "    _reorder_group(store)\n",
    "    print(f\"Completed reordering for Zarr store at path: {zarr_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening Zarr store at path: data_neuroglancer/sem.zarr\n",
      "Processing group: /\n",
      "Found array: gt_labels\n",
      "Current axes: ['x', 'y', 'z']\n",
      "Reordering array: gt_labels\n",
      "Reordered axes: ['z', 'y', 'x']\n",
      "Found array: raw\n",
      "Current axes: ['x', 'y', 'z']\n",
      "Reordering array: raw\n",
      "Reordered axes: ['z', 'y', 'x']\n",
      "Completed reordering for Zarr store at path: data_neuroglancer/sem.zarr\n",
      "Success downloading and reordering all datasets!\n"
     ]
    }
   ],
   "source": [
    "for zarr_path in [\"sem.zarr\"]:  # Only include files that were actually downloaded\n",
    "    reorder_xyz_to_zyx(os.path.join(\"data\", zarr_path))\n",
    "\n",
    "print(\"Success downloading and reordering all datasets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm_sim_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
